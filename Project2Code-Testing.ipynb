{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2daffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A: main code ####\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, map_size, dist, side, step_limit, bound=None, f1_rate=None, f2_rate=None, \n",
    "                 r_f2=None, r_f1=None, r_0=None, r_out=None, r_close=None, r_far=None):\n",
    "        # environment paras\n",
    "        self.map_size   = map_size\n",
    "        self.dist       = dist       # distance btw robot and fire scene\n",
    "        self.side       = side       # side length of the square fire scene\n",
    "        self.f1_rate    = f1_rate    # level 1 fire rate\n",
    "        self.f2_rate    = f2_rate    # level 2 fire rate\n",
    "        self.rows       = self.map_size[0]\n",
    "        self.cols       = self.map_size[1]\n",
    "        self.case_map   = None       # saved fire map \n",
    "        self.curr_map   = None       # fire map without agent\n",
    "        self.cell_size  = self.side/self.rows # size of discrete cell\n",
    "        self.origin     = [self.side/2, self.side/2]  # initial position of robot\n",
    "        self.ori_state  = [int(self.rows//2), int(self.cols//2)]  # initial state of robot\n",
    "        self.done       = False      # whether an episode is done\n",
    "        # reward\n",
    "        self.r_f2       = r_f2       # reward of reaching level 2 fire\n",
    "        self.r_f1       = r_f1       # reward of reaching level 1 fire\n",
    "        self.r_0        = r_0        # reward of reaching \"0\" states\n",
    "        self.r_out      = r_out      # out-of-range reward\n",
    "        self.r_close    = r_close    # reward of an action going towards the goal\n",
    "        self.r_far      = r_far      # reward of an action not going towards the goal\n",
    "        # fire\n",
    "        self.f1_num   = int(self.rows*self.cols*self.f1_rate)\n",
    "        self.f2_num   = int(self.rows*self.cols*self.f2_rate)\n",
    "        self.f_num    = self.f1_num+self.f2_num # number of fires\n",
    "        self.f_done   = 0 # number of fires being put out\n",
    "        self.f_pos    = None\n",
    "        self.f1_pos   = None\n",
    "        self.f2_pos   = None\n",
    "        # agent\n",
    "        self.curr_pos   = self.origin    # continuous state\n",
    "        self.curr_state = self.ori_state # discrete state\n",
    "        self.angle_x    = 0              # angle along col\n",
    "        self.angle_y    = 0              # angle along row\n",
    "        self.step_num   = 0\n",
    "        self.agent_pv   = 10             # display value of the agent in the map\n",
    "        self.bound      = bound          # angle limit\n",
    "        self.step_limit = step_limit\n",
    "        self.target     = None           # local target\n",
    "        self.target_d   = None\n",
    "\n",
    "    def create_map(self, f=None):\n",
    "        if f == None:\n",
    "            # create fires\n",
    "            self.f_pos = np.random.choice(self.rows*self.cols, size=self.f1_num+self.f2_num, replace=False)\n",
    "            self.f1_pos = np.random.choice(self.f_pos, size=self.f1_num, replace=False)\n",
    "            self.f2_pos = list((i for i in self.f_pos if i not in self.f1_pos))\n",
    "        else:\n",
    "            self.f_pos = np.array(f)\n",
    "            self.f1_pos = np.random.choice(self.f_pos, size=self.f1_num, replace=False)\n",
    "            self.f2_pos = list((i for i in self.f_pos if i not in self.f1_pos))\n",
    "        # create map\n",
    "        m = np.zeros(self.map_size)\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.rows*i+j in self.f1_pos:\n",
    "                    m[i][j] = int(1)\n",
    "                elif self.rows*i+j in self.f2_pos:\n",
    "                    m[i][j] = int(2)\n",
    "                else:\n",
    "                    m[i][j] = int(0)\n",
    "        self.case_map = m\n",
    "        self.curr_map = self.case_map.copy()\n",
    "        \n",
    "    def make_obs(self): # return a map with agent position\n",
    "        m = self.curr_map.copy()\n",
    "        m[self.curr_state[0]][self.curr_state[1]] = self.agent_pv\n",
    "        return m\n",
    "    \n",
    "    def find_nearest(self, grid, state): # find a current local nearest goal\n",
    "        min_dist = 999\n",
    "        goal = None\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if grid[i][j] != 0:\n",
    "                    dist = ((i-state[0])**2+(j-state[1])**2)**(1/2)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        goal = [i, j]\n",
    "        if goal == None:\n",
    "            print(\"Could not find a goal.\")\n",
    "            sys.exit()\n",
    "        return goal, min_dist\n",
    "        \n",
    "    def step(self, mu): \n",
    "        self.step_num += 1\n",
    "        state = self.curr_state\n",
    "        pos = self.curr_pos\n",
    "        \n",
    "        # action\n",
    "        action = mu  # mu = actor network output\n",
    "        action = action*self.bound\n",
    "        action = action.tolist() # [-bound, bound]\n",
    "        \n",
    "        # next angle to be updated\n",
    "        ay = self.angle_y + action[0] \n",
    "        ax = self.angle_x + action[1]\n",
    "        # next distance from upper-left position\n",
    "        y = self.origin[0]+self.dist*math.tan(ay*math.pi/180) \n",
    "        x = self.origin[1]+self.dist*math.tan(ax*math.pi/180)\n",
    "        # next coordinates from upper-left state\n",
    "        s1 = int(y//self.cell_size) \n",
    "        s2 = int(x//self.cell_size)\n",
    "        state_ = [s1, s2]\n",
    "        pos_ = [y, x]\n",
    "        \n",
    "        # check validation, move, get reward\n",
    "        if y<0 or y>self.side or x<0 or x>self.side:\n",
    "#             print(\"OUT OF RANGE\")\n",
    "            reward = self.r_out\n",
    "        else:\n",
    "            # move\n",
    "            self.curr_pos = pos_\n",
    "            self.curr_state = [s1,s2]\n",
    "            self.angle_y += action[0]\n",
    "            self.angle_x += action[1]\n",
    "            # get reward\n",
    "            if self.target == None:\n",
    "                temp = (np.array(pos)/self.cell_size).tolist()\n",
    "                goal_state, goal_dist = self.find_nearest(self.curr_map, temp)\n",
    "                self.target = goal_state\n",
    "            curr_dist = ((pos[0]/self.cell_size-self.target[0])**2+\n",
    "                         (pos[1]/self.cell_size-self.target[1])**2)**(1/2)\n",
    "            next_dist = ((pos_[0]/self.cell_size-self.target[0])**2+\n",
    "                         (pos_[1]/self.cell_size-self.target[1])**2)**(1/2)\n",
    "            reward = (curr_dist-next_dist)*10\n",
    "            # reset target when it's found\n",
    "            if state_ == self.target:\n",
    "                self.target = None\n",
    "            # put out fire\n",
    "            if self.curr_map[s1][s2] != 0:\n",
    "                self.curr_map[s1][s2] = 0\n",
    "                self.f_done += 1\n",
    "        \n",
    "        self.render()\n",
    "        print(\"Step:   \", self.step_num)\n",
    "        print(\"State:  \", state)\n",
    "        print(\"Target: \", self.target)\n",
    "        print(\"Action: \", action)\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        # check done\n",
    "        if self.f_done == self.f_num or self.step_num == self.step_limit:\n",
    "            self.done = True\n",
    "        \n",
    "        return self.make_obs(), reward, self.done\n",
    "        \n",
    "    def render(self):\n",
    "        m = self.make_obs()\n",
    "        print(m)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.curr_map = self.case_map.copy()\n",
    "        self.curr_state = self.ori_state\n",
    "        self.curr_pos = self.origin\n",
    "        self.angle_x = 0\n",
    "        self.angle_y = 0\n",
    "        self.f_done = 0\n",
    "        self.done = False\n",
    "        self.step_num = 0\n",
    "        self.target = None\n",
    "        obs = self.make_obs()\n",
    "        return obs\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - done # to facilitate the bellman update equation\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        return states, actions, rewards, states_, terminal\n",
    "    \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, lr_c, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        f1 = 0.003\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1) # initialize weights & bias in a narrow range\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2 = 0.003\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2.bias.data.uniform_(-f2, f2)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        f3 = 0.003\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "        self.q.weight.data.uniform_(-f3, f3)\n",
    "        self.q.bias.data.uniform_(-f3, f3)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr_c)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        action_value = F.relu(self.fc3(action))\n",
    "        state_action_value = F.relu(T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, lr_a, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        \n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(self.input_dims, self.fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.fc1_dims, self.fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.fc2_dims, self.n_actions),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr_a)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.stack(state)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, lr_a, lr_c, input_dims, tau, env, gamma, n_actions, \n",
    "                 max_size, layer1_size, layer2_size, batch_size):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.input_dims = input_dims\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.actor = ActorNetwork(lr_a, input_dims, layer1_size, layer2_size, \n",
    "                                  n_actions=n_actions, name='Actor')\n",
    "        self.critic = CriticNetwork(lr_c, input_dims, layer1_size, layer2_size, \n",
    "                                    n_actions=n_actions, name='Critic')\n",
    "        self.target_actor = ActorNetwork(lr_a, input_dims, layer1_size, layer2_size, \n",
    "                                         n_actions=n_actions, name='TargetActor')\n",
    "        self.target_critic = CriticNetwork(lr_c, input_dims, layer1_size, layer2_size, \n",
    "                                           n_actions=n_actions, name='TargetCritic')\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "        self.loss_a = []\n",
    "        self.loss_c = []\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        self.actor.eval()\n",
    "        obs = T.tensor(obs, dtype=T.float)\n",
    "        obs = obs.view(self.input_dims)\n",
    "        mu = self.actor.forward(obs)\n",
    "        mu_prime = mu + T.tensor(self.noise(), dtype=T.float)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        reward = T.tensor(reward, dtype=T.float)\n",
    "        done = T.tensor(done)\n",
    "        new_state = T.tensor(new_state, dtype=T.float)\n",
    "        action = T.tensor(action, dtype=T.float)\n",
    "        state = T.tensor(state, dtype=T.float)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "        \n",
    "        # obtain target value\n",
    "        target = []\n",
    "        for j in range(self.batch_size):\n",
    "            target.append(reward[j] + self.gamma * critic_value_[j] * done[j])\n",
    "        target = T.tensor(target)\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        # critic\n",
    "        self.critic.train() \n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value) # loss calculation\n",
    "        critic_loss.backward() # backpropagate\n",
    "        self.critic.optimizer.step() # update the network\n",
    "        self.critic.eval()\n",
    "        \n",
    "        # actor\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.loss_a.append(actor_loss)\n",
    "        self.loss_c.append(critic_loss)\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_critic_dict[name].clone()\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                     (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c88e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### B: helper functions ####\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "def flatten(x): # flatten a list\n",
    "    y = np.array(x).reshape((1,-1))\n",
    "    y = y.tolist()\n",
    "    return y[0]\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def greedy_choice(mu, eps):\n",
    "    max_index = mu.index(max(mu))\n",
    "    probs = []\n",
    "    for i in range(4):\n",
    "        if i == max_index:\n",
    "            probs.append(1-eps+eps/len(mu))\n",
    "        else:\n",
    "            probs.append(eps/len(mu))\n",
    "    return np.random.choice([0,1,2,3], p=probs)\n",
    "\n",
    "def ploting(scores, y='Total reward', x=None, window=100):   \n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    plt.ylabel(y)       \n",
    "    plt.xlabel('Episodes')                     \n",
    "    plt.plot(x, running_avg)\n",
    "    \n",
    "def plotLoss(scores, filename, x=None, window=5):   \n",
    "    l = []\n",
    "    for i in range(len(scores)):\n",
    "        l.append(scores[i].tolist())\n",
    "    plt.ylabel('Loss')       \n",
    "    plt.xlabel('Episodes')                     \n",
    "    plt.plot(l)\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "def plot_tensor(x):\n",
    "    l = []\n",
    "    for i in range(len(x)):\n",
    "        if i % 20 == 0:\n",
    "            l.append(x[i].tolist()) \n",
    "    ploting(l, y=\"Loss\", window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0df315e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 0. 2.]\n",
      " [0. 0. 2. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#### C: initialization ####\n",
    "\n",
    "# environment paras\n",
    "MAP_SIZE     = (5,5)   # [row_n, col_n]\n",
    "DISTANCE     = 5       # distance between robot and fire scene\n",
    "SIDE_LENGTH  = 15      # side length of the square scene\n",
    "F1_RATE      = 0       # level 1 fire rate\n",
    "F2_RATE      = 0.2     # level 2 fire rate\n",
    "# STEP_LIMIT   = MAP_SIZE[0]*MAP_SIZE[1]*2\n",
    "STEP_LIMIT   = 200\n",
    "REWARD_F2    = 10      # reward of reaching level 2 fire\n",
    "REWARD_F1    = 5       # reward of reaching level 1 fire\n",
    "REWARD_0     = -1      # reward of reaching \"0\" states\n",
    "REWARD_OUT   = -1      # out-of-range reward\n",
    "R_CLOSE      = 10      # reward of an action going towards the goal\n",
    "R_FAR        = -10     # reward of an action not going towards the goal\n",
    "\n",
    "# agent paras\n",
    "LR_ACTOR     = 0.001\n",
    "LR_CRITIC    = 0.001\n",
    "TAU          = 0.001\n",
    "GAMMA        = 0.99\n",
    "MEMORY       = 10000\n",
    "BATCH        = 5\n",
    "BOUND        = 5\n",
    "\n",
    "INPUT        = MAP_SIZE[0]*MAP_SIZE[1]\n",
    "LAYER1       = INPUT*2\n",
    "LAYER2       = INPUT*2\n",
    "OUTPUT       = 2\n",
    "\n",
    "# create environment\n",
    "env = Env(map_size=MAP_SIZE, dist=DISTANCE, side=SIDE_LENGTH, bound=BOUND,\n",
    "          f1_rate=F1_RATE, f2_rate=F2_RATE, step_limit=STEP_LIMIT, \n",
    "          r_f2=REWARD_F2, r_f1=REWARD_F1, r_0=REWARD_0, r_out=REWARD_OUT, r_close=R_CLOSE, r_far=R_FAR)\n",
    "env.create_map(f=[4, 9, 18, 17, 14])\n",
    "# env.create_map() # create a random map\n",
    "# create agent\n",
    "agent = Agent(lr_a=LR_ACTOR, lr_c=LR_CRITIC, input_dims=INPUT, tau=TAU, env=env, gamma=GAMMA, \n",
    "              max_size=MEMORY, batch_size=BATCH,  layer1_size=LAYER1, layer2_size=LAYER2, n_actions=OUTPUT)\n",
    "\n",
    "reward_history = []       # list of averaged reward in an episode\n",
    "reward_history_sum = []   # list of accumulated reward in an episode\n",
    "f_done_list = []          # list of the number of goal being found in an episode\n",
    "print(env.case_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e71289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "#### D: load model ####\n",
    "\n",
    "agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbf2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0. 10.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "Step:    36\n",
      "State:   [0, 3]\n",
      "Target:  None\n",
      "Action:  [-3.988175868988037, 3.812973737716675]\n",
      "Reward:  2.1018346458106207\n"
     ]
    }
   ],
   "source": [
    "#### G: testing ####\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(obs)\n",
    "    new_obs, reward, done = env.step(action)\n",
    "    obs = new_obs\n",
    "    time.sleep(0.1) # slow down to see how it moves\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec0865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
